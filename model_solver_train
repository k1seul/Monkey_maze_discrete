import os 
import time 
import subprocess
from model_based_map import ModelBasedMap

from agent import Agent
from torch.utils.tensorboard import SummaryWriter 
import pickle 
from save_data import Save_data 
from shannon_info import shannon_info
import numpy as np 
from evaluate_fixed_agent import evaluate_fixed_agent

def agent_train():
    game_name = 'Model_solver'
    log_dir = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop/tensorboard_Data')
    data_dir = os.path.join(os.path.join(os.path.expanduser('~'), 'Desktop/model_Data/' + game_name + '/'  ))

    port = 6983 ## random.randint(6000, 7000)
    subprocess.Popen(f"tensorboard --logdir={log_dir} --port={port} --reload_multifile=true", shell=True)

    log_dir = log_dir + '/' + game_name 
    
    env = ModelBasedMap() 

    state_size = env.state_n 
    action_size = env.action_n
    simulation = False

    hidden_size = 1024 
    learning_rate = 0.001 
    memory_size = 10000 
    batch_size = 64
    gamma = 0.99 

    agent = Agent(state_size= state_size,
                  action_size= action_size,
                  hidden_size= hidden_size,
                  learning_rate= learning_rate,
                  memory_size= memory_size,
                  batch_size= batch_size,
                  gamma= gamma
                  )
    
    # Set up TensorBoard output
    writer = SummaryWriter(log_dir=log_dir)


    num_episode = 10000 


 


    for i_episode in range(num_episode):
        state, info = env.reset()

    
        
        done = False
        truncated = False 
        total_length = 1
        total_reward = 0 
        state_trajectories = [] 
        action_trajectories = [] 
        while not(done or truncated):

            action = agent.act(state)
            next_state, reward, done, truncated, info = env.step(action)
    

            total_reward += reward 
            total_length += 1



            agent.remember(state, action, reward, next_state, done)
            
            if agent.agent_memory_based & (done or truncated):
                agent.record_goal_memory(next_state, reward)


            state = next_state


            agent.replay()



        if done:
            agent.decay_epsilon()

        

        writer.add_scalar("reward", total_reward, i_episode) 
        writer.add_scalar("length", total_length, i_episode)
        writer.add_scalar("reward_rate", total_reward/total_length, i_episode)
        writer.add_scalar("epsilion", agent.epsilon, i_episode)


        Save_data(agent.q_network, i_episode, save_rate = 2000, name= data_dir + '/' + 'Q_network') 

        print("Episode: {}, total_reward: {:.2f}, epsilon: {:.2f}, length: {}".format(i_episode, total_reward, agent.epsilon, total_length))



    # Close the environment and writer
    env.close()
    writer.close()


agent_train() 
